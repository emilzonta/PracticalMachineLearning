{"name":"Emil Zonta - Practical Machine Learning Course Project","tagline":"R markdown and compiled HTML file describing my analysis for Practical Machine Learning Course Project.","body":"First of all we load the *caret* package and set the seed for reproducible results.\r\n```{r}\r\nlibrary(caret)\r\nset.seed(121212)\r\n```\r\n\r\nAnalyzing the raw data in the *pml-Training.csv* file we notice that there are a lot of **missing**, **NA**'s and **#DIV/0!** values.\r\n\r\nThen we load the data considering all of these values as NA's, to facilitate our task.\r\n```{r}\r\npmlTraining <- read.table(\"C://Users/emil.zonta/Desktop/pml-training.csv\", header=TRUE , sep=\",\" , na.strings=c(\"\",\"#DIV/0!\",\"NA\"))\r\ndim(pmlTraining)\r\n```\r\n\r\nSo we have 160 columns but investigating a few with `summary(pmlTraining)`, and looking at some portions of the data,\r\nfor example with a *unix-style* command like `tail(head(pmlTraining,n=29),n=10)`,\r\nwe easily find that there are 19216 rows s.t. `new_window == 'no'` and this is exactly the amount of NA's in many columns:\r\nthese columns have a non-missing value only correspondingly to `new_window == 'yes'`.\r\n\r\nWe don't want to discard rows since some of their columns are useful, but we discard all of these columns with too many NA's\r\nsince their meaningless for prediction. We filter our data using 19216 as a threshold:\r\n```{r}\r\npmlTraining <- pmlTraining[,colSums(is.na(pmlTraining)) < 19216]\r\ndim(pmlTraining)\r\n```\r\n\r\nThere are 60 columns left and now we just want to be sure that there are no remaining rows with NA's.\r\nWe verify that all the rows are complete using the following combination of `any` and `is.na`:\r\n```{r}\r\nrowWithNAs <- apply(pmlTraining, 1, function(x){any(is.na(x))})\r\nsum(rowWithNAs)\r\n```\r\nAs shown the number of remaining rows with NA's is now equal to **0**.\r\n\r\nWe still need to do some data-cleaning since there still are some useless columns like the already mentioned \r\n*new_window*, *num_window*, the index *X*, *user_name* and *timestamps*.\r\nAll of these columns have nothing to do with our prediction problem and we need to avoid them.\r\n```{r}\r\npmlTraining <- subset(pmlTraining,select=-c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window,num_window))\r\ndim(pmlTraining)\r\n```\r\n\r\nFinally we have our **52** predictors and the outcome *classe*, which already is a factor:\r\n```{r}\r\nis.factor(pmlTraining$classe)\r\n```\r\n\r\nOk, now that our data is clean we can proceed splitting in training and testing sets.\r\n```{r}\r\ninTrain  <- createDataPartition(pmlTraining$classe, p=0.7, list=FALSE)\r\ntraining <- pmlTraining[inTrain,]\r\ntesting  <- pmlTraining[-inTrain,]\r\n```\r\n\r\nBefore fitting with `train()`, in order to speed it up, it's useful to set some *computational nuances options*\r\nwith the `trainControl()` function of *caret* package. \r\nWe set the method to be *cross-validation* and 3 as the number of folds, which appears to be enough.\r\n```{r}\r\nspeedUp <- trainControl(method=\"cv\", number=3)\r\n```\r\n\r\nSo we are ready to make our first trial using *Random Forests*. We also evaluate processing time.\r\n```{r}\r\ninitTime   <- proc.time()\r\nmodFit     <- train(training$classe ~ ., data=training, model=\"rf\", trControl=speedUp)\r\ntrainTime  <- proc.time() - initTime\r\ntrainTime\r\n```\r\n\r\nLet's look at the model:\r\n```{r}\r\nmodFit$finalModel\r\n```\r\n\r\nThe model looks very accurate, but we need to cross-validate it using the testing partition.\r\n```{r}\r\ntestingPredict <- predict(modFit, newdata=testing)\r\nconfusionMatrix(testing$classe, testingPredict)\r\n```\r\n\r\nHence the accuracy is pretty high, and so the out-of-sample error,\r\ni.e. the total amount of wrong predictions in the testing set related to the number of its elements, is particularly small:\r\n```{r}\r\ntesting$predWrong <- testingPredict != testing$classe\r\nsum(testing$predWrong) / length(testingPredict)\r\n```\r\n\r\nWe can also try to use `randomForest()` from the *randomForest* package, hoping for best perfomances (as suggested in the discussion forum).\r\nSimilarly to what we've done with `trainControl()`, we use `tuneRF()` to understand the optimal setting.\r\n```{r}\r\nlibrary(randomForest)\r\nspeedUpRF <- tuneRF(training[-53],training$classe, ntreeTry=100,stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE, dobest=FALSE)\r\n```\r\n\r\nSo we make our second trial using `randomForest()`, and according to this output we set `mtry=10`, again saving the processing time.\r\n```{r}\r\ninitTime   <- proc.time()\r\nmodFitRF <-randomForest(classe~.,data=training, mtry=10, ntree=100,keep.forest=TRUE, importance=TRUE,test=testing)\r\nrandomForestTime  <- proc.time() - initTime\r\nrandomForestTime\r\n```\r\n\r\nIt turns out that this alternative method is `r paste(round(trainTime[3]/randomForestTime[3],1))` times faster than the use that we made of `train()`!\r\n\r\nAgain, just to be sure that this new model is good, we check the out-of-sample error.\r\n```{r}\r\ntestingPredictRF <- predict(modFitRF, newdata=testing)\r\nconfusionMatrix(testing$classe, testingPredictRF)\r\ntesting$predWrongRF <- testingPredictRF != testing$classe\r\nsum(testing$predWrongRF) / length(testingPredictRF)\r\n```\r\n\r\nThe error rate is even smaller!\r\n\r\nWe conclude the analysis plotting a summary of the variable importance, according to mean decrease in accuracy and mean decrease in Gini.\r\n```{r, fig.align='center', fig.height=9, fig.width=12}\r\nvarImpPlot(modFitRF)\r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}