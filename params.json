{"name":"Practical Machine Learning Course Project","tagline":"R markdown and compiled HTML file describing my analysis for Practical Machine Learning Course Project.","body":"---\r\ntitle: \"Emil Zonta - Practical Machine Learning Course Project\"\r\noutput:\r\n  html_document:\r\n    toc: true\r\n    theme: united\r\n---\r\n\r\nFirst of all we load the only package that we need and set the seed for reproducible results.\r\n```{r}\r\nlibrary(caret)\r\nset.seed(121212)\r\n```\r\n\r\nAnalyzing the raw data in the *pml-Training.csv* file we notice that there are a lot of **missing**, **NA**'s and **#DIV/0!** values.\r\n\r\nThen we load the data considering all of these values as NA's, to facilitate our task.\r\n```{r}\r\npmlTraining <- read.table(\"C://Users/emil.zonta/Desktop/pml-training.csv\", header=TRUE , sep=\",\" , na.strings=c(\"\",\"#DIV/0!\",\"NA\"))\r\ndim(pmlTraining)\r\n```\r\n\r\nSo we have 160 columns but investigating a few with `summary(pmlTraining)`, and looking at some portions of the data,\r\nfor example with a *unix-style* command like `tail(head(pmlTraining,n=29),n=10)`,\r\nwe easily find that there are 19216 rows s.t. `new_window == 'no'` and this is exactly the amount of NA's in many columns:\r\nthese columns have a non-missing value only correspondingly to `new_window == 'yes'`.\r\n\r\nWe don't want to discard rows since some of their columns are useful, but we discard all of these columns with too many NA's\r\nsince their meaningless for prediction. We filter our data using 19216 as a threshold:\r\n```{r}\r\npmlTraining <- pmlTraining[,colSums(is.na(pmlTraining)) < 19216]\r\ndim(pmlTraining)\r\n```\r\n\r\nThere are 60 columns left and now we just want to be sure that there are no remaining rows with NA's.\r\nWe verify that all the rows are complete using the following combination of `any` and `is.na`:\r\n```{r}\r\nrowWithNAs <- apply(pmlTraining, 1, function(x){any(is.na(x))})\r\nsum(rowWithNAs)\r\n```\r\nAs shown the number of remaining rows with NA's is now equal to **0**.\r\n\r\nWe still need to do some data-cleaning since there still are some useless columns like the already mentioned \r\n*new_window*, *num_window*, the index *X*, *user_name* and *timestamps*.\r\nAll of these columns have nothing to do with our prediction problem and we need to avoid them.\r\n```{r}\r\npmlTraining <- subset(pmlTraining,select=-c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window,num_window))\r\ndim(pmlTraining)\r\n```\r\n\r\nFinally we have our **52** predictors and the outcome *classe*, which already is a factor:\r\n```{r}\r\nis.factor(pmlTraining$classe)\r\n```\r\n\r\nOk, now that our data is clean we can proceed splitting in training and testing sets.\r\n```{r}\r\ninTrain  <- createDataPartition(pmlTraining$classe, p=0.7, list=FALSE)\r\ntraining <- pmlTraining[inTrain,]\r\ntesting  <- pmlTraining[-inTrain,]\r\n```\r\n\r\nBefore fitting with `train()`, in order to speed it up, it's useful to set some *computational nuances options*\r\nwith the `trainControl()` function of *caret* package. \r\nWe set the method to be *cross-validation* and 3 as the number of folds, which appears to be enough.\r\n```{r}\r\nspeedUp <- trainControl(method=\"cv\", number=3)\r\n```\r\n\r\nSo we are ready to make our first trial using *Random Forests*. We also evaluate processing time.\r\n```{r}\r\ninitTime   <- proc.time()\r\nmodFit     <- train(training$classe ~ ., data=training, model=\"rf\", trControl=speedUp)\r\ntrainTime  <- proc.time() - initTime; trainTime\r\n```\r\n\r\nLet's look at the model:\r\n```{r}\r\nmodFit$finalModel\r\n```\r\n\r\nThe model looks very accurate, but we need to cross-validate it using the testing partition.\r\n```{r}\r\ntestingPredict <- predict(modFit, newdata=testing)\r\nconfusionMatrix(testing$classe, testingPredict)\r\n```\r\n\r\nHence the accuracy is pretty high, and so the out-of-sample error,\r\ni.e. the total amount of wrong predictions in the testing set, related to its length, is particularly small:\r\n```{r}\r\ntesting$predWrong <- testingPredict != testing$classe\r\nsum(testing$predWrong) / length(testingPredict)\r\n```\r\n\r\nHere we have a list of the most important predictors:\r\n```{r}\r\nvarImp(modFit)\r\n```\r\n\r\nWe show as an example a plot of the two most important predictors which puts in evidence where the prediction is true or false.\r\n```{r, eval=FALSE}\r\nqplot(roll_belt,yaw_belt,colour=predWrong,data=testing)\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}