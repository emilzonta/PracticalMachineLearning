---
title: "Emil Zonta - Practical Machine Learning Course Project"
output:
  html_document:
    toc: true
    theme: united
---

First of all we load the only package that we need and set the seed for reproducible results.
```{r}
library(caret)
set.seed(121212)
```

Analyzing the raw data in the *pml-Training.csv* file we notice that there are a lot of **missing**, **NA**'s and **#DIV/0!** values.

Then we load the data considering all of these values as NA's, to facilitate our task.
```{r}
pmlTraining <- read.table("C://Users/emil.zonta/Desktop/pml-training.csv", header=TRUE , sep="," , na.strings=c("","#DIV/0!","NA"))
dim(pmlTraining)
```

So we have 160 columns but investigating a few with `summary(pmlTraining)`, and looking at some portions of the data,
for example with a *unix-style* command like `tail(head(pmlTraining,n=29),n=10)`,
we easily find that there are 19216 rows s.t. `new_window == 'no'` and this is exactly the amount of NA's in many columns:
these columns have a non-missing value only correspondingly to `new_window == 'yes'`.

We don't want to discard rows since some of their columns are useful, but we discard all of these columns with too many NA's
since their meaningless for prediction. We filter our data using 19216 as a threshold:
```{r}
pmlTraining <- pmlTraining[,colSums(is.na(pmlTraining)) < 19216]
dim(pmlTraining)
```

There are 60 columns left and now we just want to be sure that there are no remaining rows with NA's.
We verify that all the rows are complete using the following combination of `any` and `is.na`:
```{r}
rowWithNAs <- apply(pmlTraining, 1, function(x){any(is.na(x))})
sum(rowWithNAs)
```
As shown the number of remaining rows with NA's is now equal to **0**.

We still need to do some data-cleaning since there still are some useless columns like the already mentioned 
*new_window*, *num_window*, the index *X*, *user_name* and *timestamps*.
All of these columns have nothing to do with our prediction problem and we need to avoid them.
```{r}
pmlTraining <- subset(pmlTraining,select=-c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window,num_window))
dim(pmlTraining)
```

Finally we have our **52** predictors and the outcome *classe*, which already is a factor:
```{r}
is.factor(pmlTraining$classe)
```

Ok, now that our data is clean we can proceed splitting in training and testing sets.
```{r}
inTrain  <- createDataPartition(pmlTraining$classe, p=0.7, list=FALSE)
training <- pmlTraining[inTrain,]
testing  <- pmlTraining[-inTrain,]
```

Before fitting with `train()`, in order to speed it up, it's useful to set some *computational nuances options*
with the `trainControl()` function of *caret* package. 
We set the method to be *cross-validation* and 3 as the number of folds, which appears to be enough.
```{r}
speedUp <- trainControl(method="cv", number=3)
```

So we are ready to make our first trial using *Random Forests*. We also evaluate processing time.
```{r}
initTime   <- proc.time()
modFit     <- train(training$classe ~ ., data=training, model="rf", trControl=speedUp)
trainTime  <- proc.time() - initTime; trainTime
```

Let's look at the model:
```{r}
modFit$finalModel
```

The model looks very accurate, but we need to cross-validate it using the testing partition.
```{r}
testingPredict <- predict(modFit, newdata=testing)
confusionMatrix(testing$classe, testingPredict)
```

Hence the accuracy is pretty high, and so the out-of-sample error,
i.e. the total amount of wrong predictions in the testing set, related to its length, is particularly small:
```{r}
testing$predWrong <- testingPredict != testing$classe
sum(testing$predWrong) / length(testingPredict)
```

Here we have a list of the most important predictors:
```{r}
varImp(modFit)
```

We show as an example a plot of the two most important predictors which puts in evidence where the prediction is true or false.
```{r, eval=FALSE}
qplot(roll_belt,yaw_belt,colour=predWrong,data=testing)
```